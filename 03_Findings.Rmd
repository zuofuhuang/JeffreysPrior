# Findings: Jeffreys Prior

Let $\theta$ be a parameter of a distribution that model a random variable $X$:

$\pi(\theta)$ denotes the prior of $\theta$;

$\pi_J(\theta)$ denotes the Jeffreys Prior of $\theta$;

## Definition and attributes

To define a prior that is transformation invariant, Harold Jeffreys proposed taking the prior distribution on parameter space that is proportional to the square root of the determinant of Fisher information (Liu and Wasserman, 2014),
$$\pi_J (\theta) \propto (I(\theta))^\frac{1}{2}$$

where $I(\theta)$ is the Fisher information, as explained above:

$$I(\theta) = -E_{\theta} \left(\frac{d^2 log(f(X|\theta)}{d\theta^2}\right)$$

A few attributes are important to note here. First, a Jeffreys prior can be an improper prior. Second, Jeffreys priors are not always conjugate priors. However, they are the limits of conjugate prior densities (Jordan lecture 7). For example, a Gaussian density approaches a flat prior as $\sigma_0 \rightarrow \infty$.

## Invariance to parametrization

To show that Jeffreys prior is preferred over a commonly mistaken “noninformative” prior such as $Beta(1,1)$, we show that Jeffreys prior is invariant to reparametrization. This process includes proof of the Invariance Principle by Jeffreys (1946) and Jordan (2010).

Suppose $\pi(\theta)$ is a Jeffreys prior on $\theta$, and we define a new parameter $\phi = h(\theta)$ as a function of $\theta$. The question is whether $\pi_J(\theta)$ after a change of variable is the same as $\pi_J(\phi)$. To prove that, we first calculate Fisher Information of $\phi$:

$$\begin{align} I(\phi) & = -E\left[\frac{d^2 logf(X|\phi)}{d\phi^2}\right] \\ & = 
-E\left[\frac{d}{d\phi}\frac{dlogf(X|\phi)}{d\phi}\right] \\ & = 
-E\left[\frac{d}{d\phi}\frac{dlogf(X|\phi(\theta))}{d\phi}\right] \\ & =
-E\left[\frac{d}{d\phi}\left(\frac{dlogf(X|\phi(\theta))}{d\theta}·\frac{d\theta}{d\phi}\right)\right] \end{align}$$

Let's stop here for a second. Here's where the **Chain Rule** works its magic.

$$(A·B)' = A'·B + A·B'$$

&nbsp;

&nbsp;

If we think of $\frac{dlogf(X|\phi(\theta))}{d\theta}$ as part $A$ of the chain rule formula, and $\frac{d\theta}{d\phi}$ as part B, we have something we can work with:


$$\begin{align}
- E\left[\frac{dA}{d\phi}·B+\frac{dB}{d\phi}·A\right] & = 
-E\left[\frac{d^2 log f(X|\phi(\theta))}{d\theta d\phi}·\frac{d\theta}{d\phi} + \frac{d^2\theta}{d\phi^2}·\frac{dlogf(X|\theta)}{d\theta}\right] \\ & = 
-E\left[\frac{d^2 logf(X|\theta)}{d\theta d\theta}·\frac{d\theta}{d\phi}·\frac{d\theta}{d\phi} + \frac{d log f(X|\theta)}{d\theta}\frac{d^2\theta}{d\phi^2}\right] \\ & = 
-E\left[\frac{d^2 logf(X|\theta)}{d\theta^2}·\left(\frac{d\theta}{d\phi}\right)^2 + \frac{d log f(X|\theta)}{d\theta}\frac{d^2\theta}{d\phi^2}\right] \;\;\;\;\;\;\;\;\;\;\;\;\;\;\; (2) \end{align} $$

From Section 2 Equation (1), we know that

$$E\left[\frac{d log f(X|\theta)}{d\theta}\right] = 0$$

Substituting this result into Equation (2), we observe that

$$I(\phi) = I(\theta)\left(\frac{d\theta}{d\phi}\right)^2 \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; (3)$$

Take the square root of that, we get 

$$\sqrt{I(\phi)} = \sqrt{I(\theta)} \left|{\frac{d\theta}{d\phi}}\right| \;\; \text{OR} \;\; \pi_J(\phi) = \pi_J(\theta) \left|{\frac{d\theta}{d\phi}}\right| \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;   (4)$$ 

$Q.E.D.$

## Following up on Bernoulli

Suppose X is binomially distributed:

$$X\sim Bin(n,\theta) \\ f(x|\theta) = {n\choose x} \theta^x (1-\theta)^{n-x}$$

From the warmup, given that E(X)=p, we know that

$$I(\theta) = -E_{\theta} \left[ \frac{d^2 log f(x|\theta)}{d\theta^2}\right] \\ =\frac{1}{\theta (1-\theta)}$$

Then $\pi_J(\theta) = I(\theta)^{\frac{1}{2}} \propto \theta^{\frac{1}{2}}(1-\theta)^{\frac{1}{2}}$, so the Jeffreys prior has the distribution of a Beta$\left(\frac{1}{2},\frac{1}{2}\right)$ density. Below, we can see the distributions of a Beta(½, ½) and a Beta(1, 1), or flat prior.

```{r}
library(ggplot2)
library(reshape2)
```

```{r}
x <- seq(0,1,length=200)
beta_dist <- data.frame(cbind(x, dbeta(x,1,1), dbeta(x,0.5,0.5)))

colnames(beta_dist) <- c("x","a=1 b=1","a=0.5 b=0.5")

beta_dist <- melt(beta_dist,x)

g <- ggplot(beta_dist, aes(x,value, color=variable))
g+geom_line() + labs(title="Beta Distribution") + labs(x="Probability", y="density")
```

Here, we see that the Jeffreys prior compensates for the likelihood by weighting the extremes. Under the likelihood, data around $p=0.5$ has the least effect on the posterior, while data that shows a true $p=0$ or $p=1$ will have the greatest effect on the posterior. The Jeffreys prior is noninformative because it weights the opposite of the likelihood function while a flat prior would not. In this case, the Jeffreys prior happens to be a conjugate prior, though this is not always true.

## Normal!

For a univariate Normal distribution $X \sim N(\theta,\sigma^2)$,

$$\begin{align} f_X(x) & = \frac{1}{\sqrt{2\pi \sigma^2}}·e^{-\frac{(x-\theta)^2}{2\sigma^2}}
\\ Log f(X;\theta) & = -\frac{1}{2} Log(2\pi \sigma^2) - \frac{(x-\theta)^2}{2\sigma^2}\end{align}$$

Then, we take the first and second derivative:

$$\begin{align} \frac{\partial}{\partial \theta} Log f(X;\theta) & = [-\frac{x^2-2x\theta + \theta^2}{2\sigma^2}]’ = \frac{x}{\sigma^2} - \frac{\theta}{\sigma^2} \\ 
\frac{\partial^2}{\partial \theta^2} Log f(X;\theta) & = -\frac{1}{\sigma^2} \end{align}$$

From there, it's just smooth sailing:

$$I(\theta) =-E(-\frac{1}{\sigma^2}) = \frac{1}{\sigma^2}$$

&nbsp;

&nbsp;

Take a quick second (or more like 15 seconds) and calculate what the **Jeffreys Prior** is here.
 
&nbsp;

$$I(\theta) =-E(-\frac{1}{\sigma^2}) = \frac{1}{\sigma^2} \;\;\; \rightarrow \;\;\; \pi_J \propto (\sigma^2)^{-\frac{1}{2}} = \frac{1}{\sigma}$$

&nbsp;
 
&nbsp;

&nbsp;
 
&nbsp;

&nbsp;

### A little change

What if we have multiple (say, $n$) observations? How would the Jeffreys Prior change?
 
&nbsp;

**Recall:** $I_n(\theta) = n·I_1(\theta)$

$$I_n(\theta) =-E(-\frac{n}{\sigma^2}) = \frac{n}{\sigma^2} \;\;\; \rightarrow \;\;\; \pi_J \propto \left(\frac{n}{\sigma^2}\right)^{1/2} = \frac{\sqrt{n}}{\sigma}$$


## Limitations

Jeffreys priors can be applied to multi-dimensional models (based on a joint density), but the results are not as reliable. 

Intuitively, the multi-dimensional Jeffreys Prior still contains a considerable amount of information about the expected value of parameter. The area surrounded by the axis and the pdf is at infinite distance, which means we’d expect the parameter to lie further away from $0$.









