# Warm-up

**Recall:** We learned about Fisher Information $(I)$ defined when $\theta$ is unidimensional by the second derivative of the log likelihood. (Jordan M.)

$$I(\theta) = -E\left[ \frac{\partial^2}{\partial \theta^2} log f(X;\theta)| \theta\right]$$

1. Find the Fisher Information for a Bernoulli Distribution $f(X|\theta)=p^x(1-p)^{1-x}$.

&nbsp;

2. What’s important about Fisher Information?

## Hint

$$\begin{align} Log(a^2·b^3) & = Log(a^2) + Log(b^3) \\ & = 
2Log(a) + 3Log(b)
\end{align}$$

&nbsp;

&nbsp;

&nbsp;

&nbsp;

$$E(X) = \theta$$

## Answers

1. Suppose X is binomially distributed:

$$X\sim Bin(1,\theta) \\ f(x|\theta) = \theta^x (1-\theta)^{1-x}$$

Then, we follow the process:

$$log \;f(x|\theta) = x·log(\theta) + (1-x)·log(1-\theta)$$

$$\frac{d}{d \theta} log \;f(x|\theta) = \frac{x}{\theta} - \frac{1-x}{1-\theta}$$

$$\frac{d^2}{d\theta^2} log \;f(x|\theta) = -\frac{x}{\theta^2}-\frac{1-x}{(1-\theta)^2}$$

Because $E(X) = \theta$, we know that 

$$\begin{align} I(\theta) & = 
-E_{\theta} \left[ \frac{d^2}{d\theta^2} log \;f(x|\theta)\right] \\ & =
\frac{\theta}{\theta^2} + \frac{1-\theta}{(1-\theta)^2} \\ & = 
\frac{1}{\theta(1-\theta)} \end{align}$$

## Warmer-up: $I(\theta)$

In Statistics, Fisher Information measures the amount of information that an observable random variable X carries about an unknown parameter $\theta$ of a distribution that models $X$. 

Let $f(X; \theta)$ denote the probability density function (pdf) or probability mass function (pmf) of $X$ conditional on the value of $\theta$. The distribution of $f(X; \theta)$ indicates the amount of information data $X$ provides on parameter $\theta$.

Formally, the partial derivative with respect to $\theta$ of the natural logarithm of $f(X;\theta)$ is defined as the **score**. If $\theta_0$ is the true parameter, the score is $0$. 

$$\begin{align} E\left[\frac{\partial}{\partial \theta} log f(X;\theta)|\theta = \theta_0 \right] & = \int \frac{\frac{\partial}{\partial \theta} f(x;\theta)|\theta=\theta_0}{f(x;\theta_0)} f(x;\theta_0) dx \\ & = \frac{\partial}{\partial \theta} \int f(x;\theta)dx \\ & = \frac{\partial}{\partial \theta} 1 = 0 \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;   (1) \end{align}$$

Since the expected value of the score function is $0$, the variance of the score function is defined as the Fisher Information:

$$I(\theta) = E\left[ \left(\frac{\partial}{\partial \theta} log f(X;\theta)\right)^2 | \theta\right] = \int \left( \frac{\partial}{\partial \theta} log f(x;\theta)\right)^2 f(x;\theta)dx$$

If $log(f(X;\theta)$ is twice differentiable, under certain regularity conditions, we can also write the Fisher Information as: 

$$I(\theta) = -E\left[ \frac{\partial^2}{\partial \theta^2} log f(X;\theta)| \theta\right]$$

## The warmest-up: Improper Prior

Let $\pi(\theta)$ denote a prior distribution. If $\int \pi(\theta) \; d\theta = \infty$, $\pi(\theta)$ is not a valid probability density, since the prior distribution does not have a finite integral.

&nbsp;

An example of improper prior is an uninformative location prior: (Jordan 2010) 

Consider a probability distribution of density $f(X - \theta)$ where $\theta$ is a location parameter that we endow with a prior. A candidate for the prior would be $\pi(\theta) \propto 1$. If $\theta$ can take any value in $\mathbb{R}$, the flat prior is not a probability density since it does not integrate to $1$.

$$\int_{\mathbb{R}} 1 \; d\theta \rightarrow \infty$$