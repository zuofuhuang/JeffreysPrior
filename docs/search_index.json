[
["index.html", "MATH 455 Mathematical Statistics - Jeffreys Prior Chapter 1 Preface", " MATH 455 Mathematical Statistics - Jeffreys Prior Emma Heth and Zuofu Huang, Spring 2019 Macalester College Chapter 1 Preface Hello! Welcome to the bookdown of our MATH 455 Mathematical Statistics project! Hope this is a good abstract of Jeffreys Prior for you. We are enjoying knowing more about and communicating Jeffreys Prior. You are always welcome to come back to check our progress! Here’s a comic in recognition of Mathematical Statistics (especially null hypothesis). Credit to https://xkcd.com/892/ "],
["introduction.html", "Chapter 2 Introduction", " Chapter 2 Introduction In Bayesian statistics, noninformative priors have become common. Subjective priors are sometimes viewed as compromising the “objectivity” of Statistics (Yang and Berger, 1998). In other cases, there is not enough prior knowledge to establish a subjective prior or a study may not have the resources to assemble and interpret prior knowledge. In any case, establishing effective and accurate noninformative priors is crucial to reporting useful and representative posteriors. Historically, statisticians like Bayes and Laplace applied a flat, or uniform, prior (Jordan 2010). While a uniform prior seems the most obvious choice for a noninformative prior, the posterior distribution will vary depending on how the random variable is parametrized. A noninformative prior should not be affected by how the variable is expressed. (We will talk through a few examples!) Additionally, flat priors can give much information on a large scale, which we will talk later! "],
["wait-jeffreys.html", "2.1 Wait, Jeffreys?", " 2.1 Wait, Jeffreys? Sir Harold Jeffreys sought a solution to this problem. He developed a prior invariant to choice of parametrization. In practice, what is most important is not the Jeffreys prior but the “Jeffreys” posterior generated from combining the prior with the likelihood. With a Jeffreys posterior of one variable, a change-of-variables will yield the Jeffreys posterior of the new variable. This way, the Jeffreys prior gives a posterior that best represents the data, no matter how it is expressed. In scientific communication, this means different information about the same data can be expressed through change-of-variables rather than by starting over and finding a new posterior. Non-informative priors are appealing because they are flat priors in a meaningful parametrization, which allow us to conduct Bayesian inference without much influence from prior knowledge. A classic prior distribution commonly mistaken as non-informative is \\(Beta(1,1)\\). Beta distributions are widely used to measure the probability of a certain event \\(X\\) on a scale of \\((0,1)\\), whereas \\(Beta(1,1)\\) indicates that the probability is equally distributed. The flat prior distribution is \\(\\pi(\\theta) = 1\\). Since \\(\\theta\\) lies between \\(0\\) and \\(1\\), we can reparametrize \\(\\theta\\) using the log-odds ratio: \\(\\rho = log\\frac{\\theta}{1-\\theta}\\). Under this change of variables, the prior distribution \\(\\pi(\\rho)\\) shows transformation variance. This highlights the importance of finding a prior that delivers a principle of invariance. (Jordan 2010) "],
["what-do-we-do.html", "2.2 What do we do?", " 2.2 What do we do? Here, we explore the development, proof, and limitations of Jeffreys priors. We will cover: Theoretical Background Notation of Jeffreys Prior Improper Prior Fisher Information Jeffreys Prior and, hopefully, some nice graphs! Some attributes of Jeffreys Prior Invariance to parametrization Binomial example \\(X \\sim Bin(n,p)\\) and its Jeffreys Prior \\(Beta \\sim \\left(\\frac{1}{2},\\frac{1}{2} \\right)\\) Normal example \\(X \\sim N(\\mu,\\sigma^2)\\) and its Jeffreys Prior Limitations of Jeffreys priors "],
["warm-up.html", "Chapter 3 Warm-up", " Chapter 3 Warm-up Recall: We learned about Fisher Information \\((I)\\) defined when \\(\\theta\\) is unidimensional by the second derivative of the log likelihood. (Jordan M.) \\[I(\\theta) = -E\\left[ \\frac{\\partial^2}{\\partial \\theta^2} log f(X;\\theta)| \\theta\\right]\\] Find the Fisher Information for a Bernoulli Distribution \\(f(X|\\theta)=p^x(1-p)^{1-x}\\). What’s important about Fisher Information? "],
["hint.html", "3.1 Hint", " 3.1 Hint \\[\\begin{align} Log(a^2·b^3) &amp; = Log(a^2) + Log(b^3) \\\\ &amp; = 2Log(a) + 3Log(b) \\end{align}\\] \\[E(X) = \\theta\\] "],
["answers.html", "3.2 Answers", " 3.2 Answers Suppose X is binomially distributed: \\[X\\sim Bin(1,\\theta) \\\\ f(x|\\theta) = \\theta^x (1-\\theta)^{1-x}\\] Then, we follow the process: \\[log \\;f(x|\\theta) = x·log(\\theta) + (1-x)·log(1-\\theta)\\] \\[\\frac{d}{d \\theta} log \\;f(x|\\theta) = \\frac{x}{\\theta} - \\frac{1-x}{1-\\theta}\\] \\[\\frac{d^2}{d\\theta^2} log \\;f(x|\\theta) = -\\frac{x}{\\theta^2}-\\frac{1-x}{(1-\\theta)^2}\\] Because \\(E(X) = \\theta\\), we know that \\[\\begin{align} I(\\theta) &amp; = -E_{\\theta} \\left[ \\frac{d^2}{d\\theta^2} log \\;f(x|\\theta)\\right] \\\\ &amp; = \\frac{\\theta}{\\theta^2} + \\frac{1-\\theta}{(1-\\theta)^2} \\\\ &amp; = \\frac{1}{\\theta(1-\\theta)} \\end{align}\\] "],
["mathematical-background-1-itheta.html", "3.3 Mathematical Background 1: \\(I(\\theta)\\)", " 3.3 Mathematical Background 1: \\(I(\\theta)\\) In Statistics, Fisher Information measures the amount of information that an observable random variable X carries about an unknown parameter \\(\\theta\\) of a distribution that models \\(X\\). Let \\(f(X; \\theta)\\) denote the probability density function (pdf) or probability mass function (pmf) of \\(X\\) conditional on the value of \\(\\theta\\). The distribution of \\(f(X; \\theta)\\) indicates the amount of information data \\(X\\) provides on parameter \\(\\theta\\). Formally, the partial derivative with respect to \\(\\theta\\) of the natural logarithm of \\(f(X;\\theta)\\) is defined as the score. If \\(\\theta_0\\) is the true parameter, the score is \\(0\\). \\[\\begin{align} E\\left[\\frac{\\partial}{\\partial \\theta} log f(X;\\theta)|\\theta = \\theta_0 \\right] &amp; = \\int \\frac{\\frac{\\partial}{\\partial \\theta} f(x;\\theta)|\\theta=\\theta_0}{f(x;\\theta_0)} f(x;\\theta_0) dx \\\\ &amp; = \\frac{\\partial}{\\partial \\theta} \\int f(x;\\theta)dx \\\\ &amp; = \\frac{\\partial}{\\partial \\theta} 1 = 0 \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; (1) \\end{align}\\] Since the expected value of the score function is \\(0\\), the variance of the score function is defined as the Fisher Information: \\[I(\\theta) = E\\left[ \\left(\\frac{\\partial}{\\partial \\theta} log f(X;\\theta)\\right)^2 | \\theta\\right] = \\int \\left( \\frac{\\partial}{\\partial \\theta} log f(x;\\theta)\\right)^2 f(x;\\theta)dx\\] If \\(log(f(X;\\theta)\\) is twice differentiable, under certain regularity conditions, we can also write the Fisher Information as: \\[I(\\theta) = -E\\left[ \\frac{\\partial^2}{\\partial \\theta^2} log f(X;\\theta)| \\theta\\right]\\] "],
["mathematical-background-2-improper-prior.html", "3.4 Mathematical Background 2: Improper Prior", " 3.4 Mathematical Background 2: Improper Prior Let \\(\\pi(\\theta)\\) denote a prior distribution. If \\(\\int \\pi(\\theta) \\; d\\theta = \\infty\\), \\(\\pi(\\theta)\\) is not a valid probability density, since the prior distribution does not have a finite integral. An example of improper prior is an uninformative location prior: (Jordan 2010) Consider a probability distribution of density \\(f(X - \\theta)\\) where \\(\\theta\\) is a location parameter that we endow with a prior. A candidate for the prior would be \\(\\pi(\\theta) \\propto 1\\). If \\(\\theta\\) can take any value in \\(\\mathbb{R}\\), the flat prior is not a probability density since it does not integrate to \\(1\\). "],
["findings-jeffreys-prior.html", "Chapter 4 Findings: Jeffreys Prior", " Chapter 4 Findings: Jeffreys Prior Let \\(\\theta\\) be a parameter of a distribution that model a random variable \\(X\\): \\(\\pi(\\theta)\\) denotes the prior of \\(\\theta\\); \\(\\pi_J(\\theta)\\) denotes the Jeffreys Prior of \\(\\theta\\); "],
["definition-and-attributes.html", "4.1 Definition and attributes", " 4.1 Definition and attributes "],
["invariance-to-parametrization.html", "4.2 Invariance to parametrization", " 4.2 Invariance to parametrization "]
]
