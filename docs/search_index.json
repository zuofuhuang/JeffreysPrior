[
["index.html", "MATH 455 Mathematical Statistics - Jeffreys Prior Chapter 1 Preface", " MATH 455 Mathematical Statistics - Jeffreys Prior Emma Heth and Zuofu Huang, Spring 2019 Macalester College Chapter 1 Preface Hello! Welcome to the bookdown of our MATH 455 Mathematical Statistics project! Hope this is a good abstract of Jeffreys Prior for you. We are enjoying knowing more about and communicating Jeffreys Prior. You are always welcome to come back to check our progress! Here’s a comic in recognition of Mathematical Statistics (especially null hypothesis). Credit to https://xkcd.com/892/ "],
["introduction.html", "Chapter 2 Introduction", " Chapter 2 Introduction In Bayesian statistics, noninformative priors have become common. Subjective priors are sometimes viewed as compromising the “objectivity” of Statistics (Yang and Berger, 1998). In other cases, there is not enough prior knowledge to establish a subjective prior or a study may not have the resources to assemble and interpret prior knowledge. In any case, establishing effective and accurate noninformative priors is crucial to reporting useful and representative posteriors. Historically, statisticians like Bayes and Laplace applied a flat, or uniform, prior (Jordan 2010). While a uniform prior seems the most obvious choice for a noninformative prior, the posterior distribution will vary depending on how the random variable is parametrized. A noninformative prior should not be affected by how the variable is expressed. (We will talk through a few examples!) Additionally, flat priors can give much information on a large scale, which we will talk later! "],
["wait-jeffreys.html", "2.1 Wait, Jeffreys?", " 2.1 Wait, Jeffreys? Sir Harold Jeffreys sought a solution to this problem. He developed a prior invariant to choice of parametrization. In practice, what is most important is not the Jeffreys prior but the “Jeffreys” posterior generated from combining the prior with the likelihood. With a Jeffreys posterior of one variable, a change-of-variables will yield the Jeffreys posterior of the new variable. This way, the Jeffreys prior gives a posterior that best represents the data, no matter how it is expressed. In scientific communication, this means different information about the same data can be expressed through change-of-variables rather than by starting over and finding a new posterior. Non-informative priors are appealing because they are flat priors in a meaningful parametrization, which allow us to conduct Bayesian inference without much influence from prior knowledge. A classic prior distribution commonly mistaken as non-informative is \\(Beta(1,1)\\). Beta distributions are widely used to measure the probability of a certain event \\(X\\) on a scale of \\((0,1)\\), whereas \\(Beta(1,1)\\) indicates that the probability is equally distributed. The flat prior distribution is \\(\\pi(\\theta) = 1\\). Since \\(\\theta\\) lies between \\(0\\) and \\(1\\), we can reparametrize \\(\\theta\\) using the log-odds ratio: \\(\\rho = log\\frac{\\theta}{1-\\theta}\\). Under this change of variables, the prior distribution \\(\\pi(\\rho)\\) shows transformation variance. This highlights the importance of finding a prior that delivers a principle of invariance. (Jordan 2010) "],
["what-do-we-do.html", "2.2 What do we do?", " 2.2 What do we do? Here, we explore the development, proof, and limitations of Jeffreys priors. We will cover: Theoretical Background Notation of Jeffreys Prior Improper Prior Fisher Information Jeffreys Prior and, hopefully, some nice graphs! Some attributes of Jeffreys Prior Invariance to parametrization Binomial example \\(X \\sim Bin(n,p)\\) and its Jeffreys Prior \\(Beta \\sim \\left(\\frac{1}{2},\\frac{1}{2} \\right)\\) Normal example \\(X \\sim N(\\mu,\\sigma^2)\\) and its Jeffreys Prior Limitations of Jeffreys priors "],
["warm-up.html", "Chapter 3 Warm-up", " Chapter 3 Warm-up Recall: We learned about Fisher Information \\((I)\\) defined when \\(\\theta\\) is unidimensional by the second derivative of the log likelihood. (Jordan M.) \\[I(\\theta) = -E\\left[ \\frac{\\partial^2}{\\partial \\theta^2} log f(X;\\theta)| \\theta\\right]\\] Find the Fisher Information for a Bernoulli Distribution \\(f(X|\\theta)=p^x(1-p)^{1-x}\\). What’s important about Fisher Information? "],
["hint.html", "3.1 Hint", " 3.1 Hint \\[\\begin{align} Log(a^2·b^3) &amp; = Log(a^2) + Log(b^3) \\\\ &amp; = 2Log(a) + 3Log(b) \\end{align}\\] \\[E(X) = \\theta\\] "],
["answers.html", "3.2 Answers", " 3.2 Answers Suppose X is binomially distributed: \\[X\\sim Bin(1,\\theta) \\\\ f(x|\\theta) = \\theta^x (1-\\theta)^{1-x}\\] Then, we follow the process: \\[log \\;f(x|\\theta) = x·log(\\theta) + (1-x)·log(1-\\theta)\\] \\[\\frac{d}{d \\theta} log \\;f(x|\\theta) = \\frac{x}{\\theta} - \\frac{1-x}{1-\\theta}\\] \\[\\frac{d^2}{d\\theta^2} log \\;f(x|\\theta) = -\\frac{x}{\\theta^2}-\\frac{1-x}{(1-\\theta)^2}\\] Because \\(E(X) = \\theta\\), we know that \\[\\begin{align} I(\\theta) &amp; = -E_{\\theta} \\left[ \\frac{d^2}{d\\theta^2} log \\;f(x|\\theta)\\right] \\\\ &amp; = \\frac{\\theta}{\\theta^2} + \\frac{1-\\theta}{(1-\\theta)^2} \\\\ &amp; = \\frac{1}{\\theta(1-\\theta)} \\end{align}\\] "],
["warmer-up-itheta.html", "3.3 Warmer-up: \\(I(\\theta)\\)", " 3.3 Warmer-up: \\(I(\\theta)\\) In Statistics, Fisher Information measures the amount of information that an observable random variable X carries about an unknown parameter \\(\\theta\\) of a distribution that models \\(X\\). Let \\(f(X; \\theta)\\) denote the probability density function (pdf) or probability mass function (pmf) of \\(X\\) conditional on the value of \\(\\theta\\). The distribution of \\(f(X; \\theta)\\) indicates the amount of information data \\(X\\) provides on parameter \\(\\theta\\). Formally, the partial derivative with respect to \\(\\theta\\) of the natural logarithm of \\(f(X;\\theta)\\) is defined as the score. If \\(\\theta_0\\) is the true parameter, the score is \\(0\\). \\[\\begin{align} E\\left[\\frac{\\partial}{\\partial \\theta} log f(X;\\theta)|\\theta = \\theta_0 \\right] &amp; = \\int \\frac{\\frac{\\partial}{\\partial \\theta} f(x;\\theta)|\\theta=\\theta_0}{f(x;\\theta_0)} f(x;\\theta_0) dx \\\\ &amp; = \\frac{\\partial}{\\partial \\theta} \\int f(x;\\theta)dx \\\\ &amp; = \\frac{\\partial}{\\partial \\theta} 1 = 0 \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; (1) \\end{align}\\] Since the expected value of the score function is \\(0\\), the variance of the score function is defined as the Fisher Information: \\[I(\\theta) = E\\left[ \\left(\\frac{\\partial}{\\partial \\theta} log f(X;\\theta)\\right)^2 | \\theta\\right] = \\int \\left( \\frac{\\partial}{\\partial \\theta} log f(x;\\theta)\\right)^2 f(x;\\theta)dx\\] If \\(log(f(X;\\theta)\\) is twice differentiable, under certain regularity conditions, we can also write the Fisher Information as: \\[I(\\theta) = -E\\left[ \\frac{\\partial^2}{\\partial \\theta^2} log f(X;\\theta)| \\theta\\right]\\] "],
["the-warmest-up-improper-prior.html", "3.4 The warmest-up: Improper Prior", " 3.4 The warmest-up: Improper Prior Let \\(\\pi(\\theta)\\) denote a prior distribution. If \\(\\int \\pi(\\theta) \\; d\\theta = \\infty\\), \\(\\pi(\\theta)\\) is not a valid probability density, since the prior distribution does not have a finite integral. An example of improper prior is an uninformative location prior: (Jordan 2010) Consider a probability distribution of density \\(f(X - \\theta)\\) where \\(\\theta\\) is a location parameter that we endow with a prior. A candidate for the prior would be \\(\\pi(\\theta) \\propto 1\\). If \\(\\theta\\) can take any value in \\(\\mathbb{R}\\), the flat prior is not a probability density since it does not integrate to \\(1\\). \\[\\int_{\\mathbb{R}} 1 \\; d\\theta \\rightarrow \\infty\\] "],
["findings-jeffreys-prior.html", "Chapter 4 Findings: Jeffreys Prior", " Chapter 4 Findings: Jeffreys Prior Let \\(\\theta\\) be a parameter of a distribution that model a random variable \\(X\\): \\(\\pi(\\theta)\\) denotes the prior of \\(\\theta\\); \\(\\pi_J(\\theta)\\) denotes the Jeffreys Prior of \\(\\theta\\); "],
["definition-and-attributes.html", "4.1 Definition and attributes", " 4.1 Definition and attributes To define a prior that is transformation invariant, Harold Jeffreys proposed taking the prior distribution on parameter space that is proportional to the square root of the determinant of Fisher information (Liu and Wasserman, 2014), \\[\\pi_J (\\theta) \\propto (I(\\theta))^\\frac{1}{2}\\] where \\(I(\\theta)\\) is the Fisher information, as explained above: \\[I(\\theta) = -E_{\\theta} \\left(\\frac{d^2 log(f(X|\\theta)}{d\\theta^2}\\right)\\] A few attributes are important to note here. First, a Jeffreys prior can be an improper prior. Second, Jeffreys priors are not always conjugate priors. However, they are the limits of conjugate prior densities (Jordan lecture 7). For example, a Gaussian density approaches a flat prior as \\(\\sigma_0 \\rightarrow \\infty\\). "],
["invariance-to-parametrization.html", "4.2 Invariance to parametrization", " 4.2 Invariance to parametrization To show that Jeffreys prior is preferred over a commonly mistaken “noninformative” prior such as \\(Beta(1,1)\\), we show that Jeffreys prior is invariant to reparametrization. This process includes proof of the Invariance Principle by Jeffreys (1946) and Jordan (2010). Suppose \\(\\pi(\\theta)\\) is a Jeffreys prior on \\(\\theta\\), and we define a new parameter \\(\\phi = h(\\theta)\\) as a function of \\(\\theta\\). The question is whether \\(\\pi_J(\\theta)\\) after a change of variable is the same as \\(\\pi_J(\\phi)\\). To prove that, we first calculate Fisher Information of \\(\\phi\\): \\[\\begin{align} I(\\phi) &amp; = -E\\left[\\frac{d^2 logf(X|\\phi)}{d\\phi^2}\\right] \\\\ &amp; = -E\\left[\\frac{d}{d\\phi}\\frac{dlogf(X|\\phi)}{d\\phi}\\right] \\\\ &amp; = -E\\left[\\frac{d}{d\\phi}\\frac{dlogf(X|\\phi(\\theta))}{d\\phi}\\right] \\\\ &amp; = -E\\left[\\frac{d}{d\\phi}\\left(\\frac{dlogf(X|\\phi(\\theta))}{d\\theta}·\\frac{d\\theta}{d\\phi}\\right)\\right] \\end{align}\\] Let’s stop here for a second. Here’s where the Chain Rule works its magic. \\[(A·B)&#39; = A&#39;·B + A·B&#39;\\] If we think of \\(\\frac{dlogf(X|\\phi(\\theta))}{d\\theta}\\) as part \\(A\\) of the chain rule formula, and \\(\\frac{d\\theta}{d\\phi}\\) as part B, we have something we can work with: \\[\\begin{align} - E\\left[\\frac{dA}{d\\phi}·B+\\frac{dB}{d\\phi}·A\\right] &amp; = -E\\left[\\frac{d^2 log f(X|\\phi(\\theta))}{d\\theta d\\phi}·\\frac{d\\theta}{d\\phi} + \\frac{d^2\\theta}{d\\phi^2}·\\frac{dlogf(X|\\theta)}{d\\theta}\\right] \\\\ &amp; = -E\\left[\\frac{d^2 logf(X|\\theta)}{d\\theta d\\theta}·\\frac{d\\theta}{d\\phi}·\\frac{d\\theta}{d\\phi} + \\frac{d log f(X|\\theta)}{d\\theta}\\frac{d^2\\theta}{d\\phi^2}\\right] \\\\ &amp; = -E\\left[\\frac{d^2 logf(X|\\theta)}{d\\theta^2}·\\left(\\frac{d\\theta}{d\\phi}\\right)^2 + \\frac{d log f(X|\\theta)}{d\\theta}\\frac{d^2\\theta}{d\\phi^2}\\right] \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; (2) \\end{align} \\] From Section 2 Equation (1), we know that \\[E\\left[\\frac{d log f(X|\\theta)}{d\\theta}\\right] = 0\\] Substituting this result into Equation (2), we observe that \\[I(\\phi) = I(\\theta)\\left(\\frac{d\\theta}{d\\phi}\\right)^2 \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; (3)\\] Take the square root of that, we get \\[\\sqrt{I(\\phi)} = \\sqrt{I(\\theta)} \\left|{\\frac{d\\theta}{d\\phi}}\\right| \\;\\; \\text{OR} \\;\\; \\pi_J(\\phi) = \\pi_J(\\theta) \\left|{\\frac{d\\theta}{d\\phi}}\\right| \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; (4)\\] \\(Q.E.D.\\) "],
["following-up-on-bernoulli.html", "4.3 Following up on Bernoulli", " 4.3 Following up on Bernoulli Suppose \\(X\\) is binomially distributed: \\[X\\sim Bin(n,\\theta) \\\\ f(x|\\theta) = {n\\choose x} \\theta^x (1-\\theta)^{n-x}\\] From the warmup, given that \\(E(X)=p\\), we know that \\[I(\\theta) = -E_{\\theta} \\left[ \\frac{d^2 log f(x|\\theta)}{d\\theta^2}\\right] \\\\ =\\frac{1}{\\theta (1-\\theta)}\\] Then \\(\\pi_J(\\theta) = I(\\theta)^{\\frac{1}{2}} \\propto \\theta^{\\frac{1}{2}}(1-\\theta)^{\\frac{1}{2}}\\), so the Jeffreys prior has the distribution of a \\(Beta\\left(\\frac{1}{2},\\frac{1}{2}\\right)\\) density. Below, we can see the distributions of a \\(Beta\\left(\\frac{1}{2},\\frac{1}{2}\\right)\\) and a \\(Beta(1, 1)\\), or flat prior. library(ggplot2) library(reshape2) x &lt;- seq(0,1,length=200) beta_dist &lt;- data.frame(cbind(x, dbeta(x,1,1), dbeta(x,0.5,0.5))) colnames(beta_dist) &lt;- c(&quot;x&quot;,&quot;a=1 b=1&quot;,&quot;a=0.5 b=0.5&quot;) beta_dist &lt;- melt(beta_dist,x) g &lt;- ggplot(beta_dist, aes(x,value, color=variable)) g+geom_line() + labs(title=&quot;Beta Distribution&quot;) + labs(x=&quot;Probability&quot;, y=&quot;density&quot;) Here, we see that the Jeffreys prior compensates for the likelihood by weighting the extremes. Under the likelihood, data around \\(p=0.5\\) has the least effect on the posterior, while data that shows a true \\(p=0\\) or \\(p=1\\) will have the greatest effect on the posterior. The Jeffreys prior is noninformative because it weights the opposite of the likelihood function while a flat prior would not. In this case, the Jeffreys prior happens to be a conjugate prior, though this is not always true. "],
["normal.html", "4.4 Normal!", " 4.4 Normal! For a univariate Normal distribution \\(X \\sim N(\\theta,\\sigma^2)\\), \\[\\begin{align} f_X(x) &amp; = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}·e^{-\\frac{(x-\\theta)^2}{2\\sigma^2}} \\\\ Log f(X;\\theta) &amp; = -\\frac{1}{2} Log(2\\pi \\sigma^2) - \\frac{(x-\\theta)^2}{2\\sigma^2}\\end{align}\\] Then, we take the first and second derivative: \\[\\begin{align} \\frac{\\partial}{\\partial \\theta} Log f(X;\\theta) &amp; = [-\\frac{x^2-2x\\theta + \\theta^2}{2\\sigma^2}]’ = \\frac{x}{\\sigma^2} - \\frac{\\theta}{\\sigma^2} \\\\ \\frac{\\partial^2}{\\partial \\theta^2} Log f(X;\\theta) &amp; = -\\frac{1}{\\sigma^2} \\end{align}\\] From there, it’s just smooth sailing: \\[I(\\theta) =-E(-\\frac{1}{\\sigma^2}) = \\frac{1}{\\sigma^2}\\] Take a quick second (or more like 15 seconds) and calculate what the Jeffreys Prior is here. \\[I(\\theta) =-E(-\\frac{1}{\\sigma^2}) = \\frac{1}{\\sigma^2} \\;\\;\\; \\rightarrow \\;\\;\\; \\pi_J \\propto (\\sigma^2)^{-\\frac{1}{2}} = \\frac{1}{\\sigma}\\] 4.4.1 A little change What if we have multiple (say, \\(n\\)) observations? How would the Jeffreys Prior change? Recall: \\(I_n(\\theta) = n·I_1(\\theta)\\) \\[I_n(\\theta) =-E(-\\frac{n}{\\sigma^2}) = \\frac{n}{\\sigma^2} \\;\\;\\; \\rightarrow \\;\\;\\; \\pi_J \\propto \\left(\\frac{n}{\\sigma^2}\\right)^{1/2} = \\frac{\\sqrt{n}}{\\sigma}\\] "],
["limitations.html", "4.5 Limitations", " 4.5 Limitations Jeffreys priors can be applied to multi-dimensional models (based on a joint density), but the results are not as reliable. Intuitively, the multi-dimensional Jeffreys Prior still contains a considerable amount of information about the expected value of parameter. The area surrounded by the axis and the pdf is at infinite distance, which means we’d expect the parameter to lie further away from \\(0\\). "],
["references.html", "Chapter 5 References", " Chapter 5 References Berger, J. (1985). Statistical Decision Theory and Bayesian Analysis, 2nd ed. Springer, New York. Gelman, Andrew, et al. (2013). Bayesian data analysis. Chapman and Hall/CRC. Good, I. (1980). The contributions of Jeffreys to Bayesian statistics. In Bayesian Analysis in Econometrics and Statistics: Essays in Honor of Harold Jeffreys 21–34. North-Holland, Amsterdam. Jeffreys, H. (1939). Theory of Probability, 1st ed. The Clarendon Press, Oxford. Jeffreys, H. (1948). Theory of Probability, 2nd ed. The Clarendon Press, Oxford. Jeffreys, H. (1961). Theory of Probability, 3rd ed. Oxford Classic Texts in the Physical Sciences. Oxford University Press, Oxford. Jordan, M. (2010). Lecture 6: Jeffreys priors [Lecture notes]. Retrieved from https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/lectures/lecture6.pdf Jordan, M. (2010). Lecture 7: Jeffreys priors and reference priors [Lecture notes]. Retrieved fromhttps://people.eecs.berkeley.edu/~jordan/courses/260-spring10/lectures/lecture7.pdf Kass, R. &amp; Wasserman, L. (1996). Formal rules of selecting prior distributions: A review and annotated bibliography. Journal of the American Statistical Association 91, 343–1370 Lindley, D. (1980). Jeffreys’s contribution to modern statistical thought. In Bayesian Analysis in Econometrics and Statistics: Essays in Honor of Harold Jeffreys 35–39. North-Holland, Amsterdam. Liu, Han &amp; Wasserman, Larry. (2014) Statistical Machine Learning, 310-312. Unpublished Manuscript, CMU. Retrieved from http://www.stat.cmu.edu/~larry/=sml/Bayes.pdf Rao, S. S. A review of results in Statistical Inference. [Lecture notes] Retrieved from http://www.stat.tamu.edu/~suhasini/teaching613/inference.pdf Robert, C. P., Chopin, N., &amp; Rousseau, J. (2009). Harold Jeffreys’s Theory of Probability Revisited. Statistical Science 24(2), 141-172. Tokdar, S. T. (2011). The Jeffreys Prior. [Lecture notes] Retrieved from https://www2.stat.duke.edu/courses/Fall11/sta114/jeffreys.pdf Yang, R. &amp; Berger, J. (1998). A catalog of noninformative priors. Durham, NC: Institute of Statistics and Decision Sciences. Retrieved from http://www.stats.org.uk/priors/noninformative/YangBerger1998.pdf "]
]
